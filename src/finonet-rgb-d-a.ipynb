{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "In [1]:"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import sys\n",
    "import sklearn\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from convlstm import ConvLSTM\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score\n",
    "\n",
    "import librosa\n",
    "from scipy.fftpack import fft\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "from keras_preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "In [3]:"
   },
   "outputs": [],
   "source": [
    "param_dict = {\n",
    "    'img_path': \"./imgs_awb/\",\n",
    "    'depth_path': \"./depth_imgs/\",\n",
    "    'audio_path': \"./audio\",\n",
    "    'img_size' : 224,\n",
    "    \n",
    "    'sampling_mode': \"depth\",\n",
    "    'n_sample_frames': 8,\n",
    "    'depth_thresh' : 500,\n",
    "    \n",
    "    'lr': 0.000001,\n",
    "    'n_epoch': 1000, \n",
    "    'batch_size': 16,    \n",
    "        \n",
    "    'batch_norm': True,    \n",
    "    'normalize': False,\n",
    "    \n",
    "    'dropout': True,\n",
    "    'dropout_p': 0.4,\n",
    "    \n",
    "    'jitter': True,\n",
    "    'j_brightness': 0.2, \n",
    "    'j_contrast': 0.2, \n",
    "    'j_saturation': 0.2,\n",
    "    'j_hue': 0.2,  \n",
    "    \n",
    "    'random_crop': False,\n",
    "    'flip': True,\n",
    "    \n",
    "    \"freeze_rgbd\": False,\n",
    "    \"freeze_arm\": True,\n",
    "}\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2,
    "scrolled": false,
    "title": "In [4]:"
   },
   "outputs": [],
   "source": [
    "# Read data directories\n",
    "\n",
    "# main data dict\n",
    "data = []\n",
    "\n",
    "id_counter = 0\n",
    "\n",
    "action_list = next(os.walk(param_dict['img_path']))[1]\n",
    "\n",
    "for action in action_list: \n",
    "    \n",
    "    # Read Label file\n",
    "    annotation_df = pd.read_csv(os.path.join(param_dict[\"img_path\"], action, \"annotation.txt\"))        \n",
    "    annotation_df[\"name\"] = np.array([int(item[0]) for item in annotation_df[\"name\"].str.split(\"_\").tolist()])        \n",
    "    \n",
    "    # Get bags list of the current action\n",
    "    action_path = os.path.join(param_dict['img_path'], action)    \n",
    "    bags = next(os.walk(action_path))[1]\n",
    "    \n",
    "    for bag in bags:\n",
    "        bag_label = int(annotation_df[annotation_df[\"name\"]==int(bag)][\"label\"])\n",
    "        fail_timestamp = float(annotation_df[annotation_df[\"name\"]==int(bag)][\"start\"])\n",
    "                \n",
    "        bag_path = os.path.join(action_path, bag)\n",
    "        \n",
    "        bag_imgs_path = glob.glob(os.path.join(bag_path, \"*.png\"))\n",
    "        bag_imgs_path.sort()\n",
    "        \n",
    "        bag_stamps_path = glob.glob(os.path.join(bag_path, \"*.txt\"))        \n",
    "        bag_stamps_path.sort()\n",
    "        \n",
    "        bag_content = []\n",
    "        \n",
    "        for img_path, stamp_path in zip(bag_imgs_path, bag_stamps_path):  \n",
    "                        \n",
    "            # Read time stamp of the frame from file\n",
    "            timestamp = float(open(stamp_path, \"r\").read())                                       \n",
    "\n",
    "            depth_img_path = os.path.join(param_dict['depth_path'], \n",
    "                              action, \n",
    "                              bag, \n",
    "                              os.path.basename(img_path).split(\".\")[0] + \".tiff\")\n",
    "            \n",
    "            d_img = Image.open(depth_img_path)\n",
    "            d_mean = np.mean(d_img)\n",
    "            d_img.close()\n",
    "            \n",
    "            bag_content.append({                \n",
    "                'img_id': os.path.basename(img_path),                \n",
    "                'timestamp':  timestamp,\n",
    "                'img_path': img_path,    \n",
    "                'img_label': bag_label & int((timestamp >= fail_timestamp)),  \n",
    "                'depth_path': depth_img_path,\n",
    "                'depth_avg': d_mean,\n",
    "            })\n",
    "        \n",
    "        \n",
    "        if os.path.isfile(os.path.join(param_dict['audio_path'], action, bag+\".wav\")):\n",
    "        # Prep data dict\n",
    "            data.append({\n",
    "                'unique_id': id_counter,\n",
    "                'action': action,\n",
    "                'bag_no': int(bag),            \n",
    "                'bag_label': int(bag_label),  \n",
    "                'bag_content': bag_content,\n",
    "                'audio_path': os.path.join(param_dict['audio_path'], action, bag+\".wav\"),\n",
    "            })\n",
    "\n",
    "            id_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "In [5]:"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "In [6]:"
   },
   "outputs": [],
   "source": [
    "# Train / test split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_all_data():\n",
    "    tr_succ, tr_fail, te_succ, te_fail = [], [], [], []\n",
    "    \n",
    "    for action in df[\"action\"].unique():\n",
    "        succ = df.loc[(df['action'] == action) & (df['bag_label'] == 0)][\"unique_id\"].values\n",
    "        fail = df.loc[(df['action'] == action) & (df['bag_label'] == 1)][\"unique_id\"].values\n",
    "\n",
    "        X_tr, X_te = train_test_split(succ, test_size=0.3, shuffle=True, random_state=42)\n",
    "\n",
    "        tr_succ = np.concatenate((tr_succ, X_tr), axis=0)\n",
    "        te_succ = np.concatenate((te_succ, X_te), axis=0)\n",
    "\n",
    "        X_tr, X_te = train_test_split(fail, test_size=0.3, shuffle=True, random_state=42)\n",
    "\n",
    "        tr_fail = np.concatenate((tr_fail, X_tr), axis=0)\n",
    "        te_fail = np.concatenate((te_fail, X_te), axis=0)\n",
    "        \n",
    "    return tr_succ, tr_fail, te_succ, te_fail\n",
    "\n",
    "    \n",
    "\n",
    "tr_succ, tr_fail, te_succ, te_fail = get_all_data()\n",
    "    \n",
    "train_idx = np.concatenate((tr_succ, tr_fail), axis=0)\n",
    "test_idx = np.concatenate((te_succ, te_fail), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "In [7]:"
   },
   "outputs": [],
   "source": [
    "# Data Loader\n",
    "\n",
    "tf_list = []\n",
    "\n",
    "if param_dict[\"random_crop\"]:\n",
    "    tf_list.append(transforms.RandomCrop((224,224)))    \n",
    "    \n",
    "tf_list.append(transforms.ToTensor())    \n",
    "\n",
    "if param_dict[\"normalize\"]:\n",
    "    tf_list.append(transforms.Normalize([0.47264798, 0.47641314, 0.46798028], [0.07805742, 0.0770264 , 0.08050214])) #imagenet values\n",
    "\n",
    "tf = transforms.Compose(tf_list)\n",
    "\n",
    "\n",
    "class DatasetFD(Dataset):\n",
    "    \n",
    "    def __init__(self, data, indices, n_sample_frames, sampling_mode):\n",
    "        self.data = data # main data dict\n",
    "        self.n_sample_frames = n_sample_frames  # number of frames to be sampled from each bag\n",
    "        self.sampling_mode = sampling_mode\n",
    "        self.data = self.data.iloc[indices] #Update data with train/test indices\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def _get_frame_indices(self, index):\n",
    "        frames = self.data.iloc[index][\"bag_content\"]      \n",
    "        \n",
    "        n_frames = len(frames)\n",
    "                 \n",
    "        if self.sampling_mode == \"depth\" :\n",
    "            df_depth = pd.DataFrame(frames)\n",
    "            \n",
    "            def _get_indices(df_depth):\n",
    "                a = df_depth[\"depth_avg\"].values > param_dict[\"depth_thresh\"]\n",
    "                frame_idx = np.where(a == True)[0]               \n",
    "                \n",
    "                n_frames = len(frame_idx)\n",
    "\n",
    "                segments = np.split(frame_idx, [int(n_frames / 3), int(n_frames / 3)*2])\n",
    "                first_segment_idx = np.sort(np.random.permutation(segments[0])[:4])\n",
    "                last_segment_idx = np.sort(np.random.permutation(segments[2])[:4])\n",
    "                frame_idx = np.concatenate( (first_segment_idx, last_segment_idx), 0 )\n",
    "                \n",
    "                return frame_idx\n",
    "            \n",
    "            frame_idx = _get_indices(df_depth)\n",
    "            \n",
    "        return frame_idx\n",
    "    \n",
    "    def load_img(self, img_path, vertical_flip, color_transform):        \n",
    "        if param_dict[\"random_crop\"]:\n",
    "            border = (210, 150, 454, 394) # left, up, right, bottom\n",
    "        else:\n",
    "            border = (220, 160, 444, 384) # left, up, right, bottom [224x224]\n",
    "        \n",
    "        img = Image.open(img_path).crop(border)\n",
    "        \n",
    "        # depth images need to rescale 0-255\n",
    "        # img = img.convert('RGB')        \n",
    "        if \"depth\" in img_path:\n",
    "            img = img/np.max(img)\n",
    "            img = Image.fromarray(np.uint8(img*255))\n",
    "\n",
    "        \n",
    "        if vertical_flip:            \n",
    "            img= img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "            \n",
    "            \n",
    "        if color_transform is not None:\n",
    "            img = color_transform(img)\n",
    "         \n",
    "        imgarr = tf(img)\n",
    "        \n",
    "        img.close()\n",
    "        return imgarr\n",
    "\n",
    "    def load_img_arm(self, img_path, vertical_flip, color_transform):\n",
    "        # Processing for arm images                \n",
    "        img = Image.open(img_path).crop((96, 16, 544, 464)) # left, up, right, bottom\n",
    "        img = img.resize((224,224))\n",
    "        \n",
    "        if vertical_flip:            \n",
    "            img= img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "                        \n",
    "        if color_transform is not None:\n",
    "            img = color_transform(img)\n",
    "         \n",
    "        imgarr = tf(img)\n",
    "        \n",
    "        img.close()\n",
    "        return imgarr\n",
    "\n",
    "    def load_mfcc(self, wav_path):        \n",
    "        sample_rate, samples1 = wavfile.read(wav_path)\n",
    "\n",
    "        if samples1.ndim > 1:\n",
    "            mfcc = librosa.feature.mfcc(y=samples1[:,0].astype(np.float32), sr=sample_rate).T\n",
    "        else:\n",
    "            mfcc = librosa.feature.mfcc(y=samples1.astype(np.float32), sr=sample_rate).T\n",
    "\n",
    "        mfcc = sequence.pad_sequences([mfcc], maxlen=3500)[0]\n",
    "        mfcc = torch.Tensor(mfcc).float()        \n",
    "        return mfcc\n",
    "\n",
    "    \n",
    "    def __getitem__(self, index):  \n",
    "        frames = self.data.iloc[index][\"bag_content\"]\n",
    "        label = self.data.iloc[index][\"bag_label\"]   \n",
    "        \n",
    "        frame_idx = self._get_frame_indices(index)\n",
    "\n",
    "        if param_dict[\"flip\"]:\n",
    "            vert_flip = np.random.randint(2)\n",
    "        else:\n",
    "            vert_flip = False\n",
    "            \n",
    "            \n",
    "        if param_dict[\"jitter\"]:\n",
    "            color_jitter = transforms.ColorJitter(brightness=param_dict[\"j_brightness\"], \n",
    "                                          contrast=param_dict[\"j_contrast\"], \n",
    "                                          saturation=param_dict[\"j_saturation\"], \n",
    "                                          hue=param_dict[\"j_hue\"])\n",
    "            \n",
    "            color_transform = transform = transforms.ColorJitter.get_params(\n",
    "                color_jitter.brightness, color_jitter.contrast, color_jitter.saturation,\n",
    "                color_jitter.hue)\n",
    "        else:\n",
    "            color_transform = None\n",
    "\n",
    "                     \n",
    "        img_batch = torch.stack([self.load_img(frames[idx][\"img_path\"], vert_flip, color_transform) for idx in frame_idx])\n",
    "        depthimg_batch = torch.stack([self.load_img(frames[idx][\"depth_path\"], vert_flip, color_transform) for idx in frame_idx])\n",
    "                        \n",
    "        img_batch = torch.cat((img_batch, depthimg_batch), axis=1)\n",
    "        \n",
    "        ### Audio ###\n",
    "        wav_path = self.data.iloc[index][\"audio_path\"]            \n",
    "        \n",
    "        audio_batch = self.load_mfcc(wav_path)\n",
    "    \n",
    "        return self.data.iloc[index][\"unique_id\"], frame_idx, img_batch, audio_batch, label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinoNetRGBD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FinoNetRGBD, self).__init__()  \n",
    "\n",
    "        self.num_filters1 = 64\n",
    "        self.num_filters2 = 128\n",
    "        self.num_filters3 = 128        \n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(self.num_filters1)\n",
    "        self.bn11 = nn.BatchNorm2d(self.num_filters1)\n",
    "\n",
    "        self.bn2 = nn.BatchNorm2d(self.num_filters2)\n",
    "        self.bn22 = nn.BatchNorm2d(self.num_filters2)\n",
    "\n",
    "        self.bn3 = nn.BatchNorm2d(self.num_filters3)\n",
    "        self.bn33 = nn.BatchNorm2d(self.num_filters3)\n",
    "        \n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=4, out_channels=self.num_filters1, kernel_size=3)\n",
    "        self.conv11 = nn.Conv2d(in_channels=self.num_filters1, out_channels=self.num_filters1, kernel_size=3)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.relu11 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)        \n",
    "        self.convlstm1 = ConvLSTM(input_dim=self.num_filters1,\n",
    "                hidden_dim=[self.num_filters1],\n",
    "                kernel_size=(3, 3),\n",
    "                num_layers=1,\n",
    "                batch_first=True,\n",
    "                bias=True,\n",
    "                return_all_layers=False)\n",
    "\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=self.num_filters1, out_channels=self.num_filters2, kernel_size=3)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.conv22 = nn.Conv2d(in_channels=self.num_filters2, out_channels=self.num_filters2, kernel_size=3)\n",
    "        self.relu22 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.convlstm2 = ConvLSTM(input_dim=self.num_filters2,\n",
    "                hidden_dim=[self.num_filters2],\n",
    "                kernel_size=(3, 3),\n",
    "                num_layers=1,\n",
    "                batch_first=True,\n",
    "                bias=True,\n",
    "                return_all_layers=False)\n",
    "\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=self.num_filters2, out_channels=self.num_filters3, kernel_size=3)                             \n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.conv33 = nn.Conv2d(in_channels=self.num_filters3, out_channels=self.num_filters3, kernel_size=3)                             \n",
    "        self.relu33 = nn.ReLU()     \n",
    "\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)    \n",
    "\n",
    "        self.convlstm3 = ConvLSTM(input_dim=self.num_filters3,\n",
    "                hidden_dim=[self.num_filters3],\n",
    "                kernel_size=(3, 3),\n",
    "                num_layers=1,\n",
    "                batch_first=True,\n",
    "                bias=True,\n",
    "                return_all_layers=False)        \n",
    "\n",
    "\n",
    "    def forward(self, x):        \n",
    "        \n",
    "        ### BLOCK 1 ###\n",
    "        batch_size,frame_size,channel,height,width = x.size() # 8,4,3,224,224\n",
    "        x_in1 = x.view(batch_size*frame_size, channel, height, width)           \n",
    "        x = self.conv1(x_in1) # 8,4,16,220,220\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "\n",
    "        x = self.conv11(x)\n",
    "        x = self.bn11(x)\n",
    "        x = self.relu11(x)\n",
    "        x = self.pool1(x)\n",
    "        x = F.dropout(x, param_dict[\"dropout_p\"])\n",
    "        \n",
    "        x = x.view(batch_size, frame_size, self.num_filters1, x.size()[-1], x.size()[-1])\n",
    "        x = self.convlstm1(x)[0][0] # 8,4,16, 110, 110\n",
    "        batch_size,frame_size,channel,height,width = x.size()\n",
    "        x = x.view(batch_size*frame_size, channel, height, width)               \n",
    "        x_in2 = F.dropout(x, param_dict[\"dropout_p\"])\n",
    "        \n",
    "        ### BLOCK 2 ###\n",
    "        x = self.conv2(x_in2)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = F.dropout(x, param_dict[\"dropout_p\"])\n",
    "\n",
    "        x = self.conv22(x)\n",
    "        x = self.bn22(x)\n",
    "        x = self.relu22(x)\n",
    "        x = self.pool2(x)\n",
    "        x = F.dropout(x, param_dict[\"dropout_p\"])\n",
    "        \n",
    "        x = x.view(batch_size, frame_size, self.num_filters2, x.size()[-1], x.size()[-1])\n",
    "        x = self.convlstm2(x)[0][0] # 8,4, 32, 54, 54\n",
    "        batch_size,frame_size,channel,height,width = x.size()\n",
    "        x = x.view(batch_size*frame_size, channel, height, width)\n",
    "        x_in3 = F.dropout(x, param_dict[\"dropout_p\"])\n",
    "        \n",
    "        ### BLOCK 3 ###\n",
    "        x = self.conv3(x_in3)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = F.dropout(x, param_dict[\"dropout_p\"])\n",
    "\n",
    "        x = self.conv33(x)\n",
    "        x = self.bn33(x)\n",
    "        x = self.relu33(x)\n",
    "        x = self.pool3(x)\n",
    "        x = F.dropout(x, param_dict[\"dropout_p\"])\n",
    "        \n",
    "        x = x.view(batch_size, frame_size, self.num_filters3, x.size()[-1], x.size()[-1])\n",
    "        x = self.convlstm3(x)[0][0] # 8,4, 64, 24, 24\n",
    "        \n",
    "        batch_size,frame_size,channel,height,width = x.size()\n",
    "        x = x.view(batch_size*frame_size, channel, height, width)\n",
    "        x = x.view(batch_size, frame_size, self.num_filters3, x.size()[-1], x.size()[-1])\n",
    "        \n",
    "        x = x[:,-1,:] # get last frame features            \n",
    "        x = x.view(batch_size, -1)        \n",
    "        return x\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AudioCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, (32, 20))        \n",
    "        self.conv2 = nn.Conv1d(64, 64, (32, 1))                \n",
    "\n",
    "        \n",
    "    def forward(self, x):        \n",
    "        x = x.unsqueeze(1)   \n",
    "        x = self.conv_2blocks(x)        \n",
    "        return x\n",
    "    \n",
    "    def conv_2blocks(self, input):\n",
    "        conv_out = self.conv1(input) # conv_out.size() = (batch_size, out_channels, dim, 1)        \n",
    "        conv_out = F.relu(conv_out)                \n",
    "        conv_out = self.conv2(conv_out)        \n",
    "        activation = F.relu(conv_out.squeeze(3))# activation.size() = (batch_size, out_channels, dim1)\n",
    "        max_out = F.max_pool1d(activation, activation.size()[2]).squeeze(2)# maxpool_out.size() = (batch_size, out_channels)\n",
    "        return max_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2,
    "title": "In [8]:"
   },
   "outputs": [],
   "source": [
    "class FinonetRGBDA(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FinonetRGBDA, self).__init__()  \n",
    "        \n",
    "        self.rgbd = FinoNetRGBD()                \n",
    "        self.rgbd.load_state_dict(torch.load(\"finonet-rgbd.pth\"), strict=False)\n",
    "        \n",
    "        self.audio = AudioCNN()\n",
    "        \n",
    "        if param_dict[\"freeze_rgbd\"]:\n",
    "            self.rgbd.module.conv1.requires_grad = False\n",
    "            self.rgbd.module.conv11.requires_grad = False\n",
    "            self.rgbd.module.conv2.requires_grad = False\n",
    "            self.rgbd.module.conv22.requires_grad = False\n",
    "            self.rgbd.module.conv3.requires_grad = False\n",
    "            self.rgbd.module.conv33.requires_grad = False\n",
    "            self.rgbd.module.convlstm1.cell_list[0].requires_grad = False\n",
    "            self.rgbd.module.convlstm2.cell_list[0].requires_grad = False\n",
    "\n",
    "        \n",
    "        self.HIDDEN_SIZE = 128*24*24\n",
    "        self.AUDIO_SIZE = 64\n",
    "\n",
    "        self.linear1 = nn.Linear(self.HIDDEN_SIZE + self.AUDIO_SIZE, 1024)\n",
    "        self.linear2 = nn.Linear(1024, 2)\n",
    "       \n",
    "        if param_dict[\"dropout\"]:\n",
    "            self.dropout = nn.Dropout(p=param_dict[\"dropout_p\"])\n",
    "            self.dropout2 = nn.Dropout(p=param_dict[\"dropout_p\"])\n",
    "   \n",
    "        self.mp = nn.MaxPool1d(2)\n",
    "    \n",
    "    def forward(self, x, x_audio):        \n",
    "        \n",
    "        x = self.rgbd(x)\n",
    "        x_audio = self.audio(x_audio)    \n",
    "        x = torch.cat((x,x_audio), 1)\n",
    "\n",
    "        if param_dict[\"dropout\"]:\n",
    "            x = F.relu(self.linear1(x))\n",
    "            x = self.linear2(self.dropout2(x))\n",
    "            return x\n",
    "        else:\n",
    "            return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "In [9]:"
   },
   "outputs": [],
   "source": [
    "train_dataset = DatasetFD(df, train_idx, param_dict[\"n_sample_frames\"], param_dict[\"sampling_mode\"])\n",
    "train_loader = DataLoader(train_dataset, batch_size=param_dict[\"batch_size\"], shuffle=True, num_workers=16)\n",
    "\n",
    "test_dataset = DatasetFD(df, test_idx, param_dict[\"n_sample_frames\"], param_dict[\"sampling_mode\"])\n",
    "test_loader = DataLoader(test_dataset, batch_size=param_dict[\"batch_size\"], shuffle=False, num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "In [11]:"
   },
   "outputs": [],
   "source": [
    "def test_epoch(model, test_loader, epoch):\n",
    "    model.eval()\n",
    "    \n",
    "    test_loss = 0\n",
    "    \n",
    "    all_labels, all_preds = [], []     \n",
    "    \n",
    "    for idx, frame_idx, imgs, audio, labels in test_loader:\n",
    "\n",
    "        all_labels = np.concatenate((all_labels, labels.cpu().data.numpy()), axis=0)\n",
    "\n",
    "        imgs = imgs.to(device)\n",
    "        audio = audio.to(device)\n",
    "        labels = labels.to(device) \n",
    "\n",
    "\n",
    "        output = model(imgs, audio)          \n",
    "        loss = F.cross_entropy(output, labels)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        values, indices = torch.max(torch.softmax(output, dim=1), 1)   \n",
    "\n",
    "        all_preds = np.concatenate((all_preds, indices.cpu().data.numpy()), axis=0)\n",
    "\n",
    "\n",
    "    test_loss = test_loss / len(test_dataset)\n",
    "    test_acc = f1_score(all_labels, all_preds,average='weighted')        \n",
    "\n",
    "    print(\"[Test] Epoch: {}, Loss: {} Acc: {}\".format(epoch, test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "In [12]:"
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, epoch): \n",
    "\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    all_labels, all_preds = [], []     \n",
    "        \n",
    "    for idx, frame_idx, imgs, audio, labels in train_loader:          \n",
    "\n",
    "        imgs = imgs.to(device)\n",
    "        audio = audio.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        all_labels = np.concatenate((all_labels, labels.cpu().data.numpy()), axis=0)\n",
    "\n",
    "        optimizer.zero_grad()   \n",
    "\n",
    "        output= model(imgs, audio)\n",
    "        loss = F.cross_entropy(output, labels)\n",
    "\n",
    "        values, indices = torch.max(torch.softmax(output, dim=1), 1)        \n",
    "        all_preds = np.concatenate((all_preds, indices.cpu().data.numpy()), axis=0)                                    \n",
    "\n",
    "        loss.backward()        \n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "\n",
    "    epoch_loss = epoch_loss / len(train_dataset) \n",
    "    tr_acc = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "\n",
    "    print(\"[Train] Epoch: {}, Loss: {} Acc: {}\".format(epoch, epoch_loss, tr_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2,
    "title": "In [13]:"
   },
   "outputs": [],
   "source": [
    "model = FinonetRGBDA()\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=param_dict[\"lr\"])\n",
    "\n",
    "for epoch in range(param_dict[\"n_epoch\"]):\n",
    "    train_epoch(model, train_loader, epoch)\n",
    "    test_epoch(model, test_loader, epoch)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python",
   "text_representation": {
    "extension": ".py",
    "format_name": "percent"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
